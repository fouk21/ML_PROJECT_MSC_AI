{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "import re\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import warnings\n",
    "import optuna\n",
    "from sklearn import svm\n",
    "from tqdm.notebook import tqdm # for a continuous progress bar style\n",
    "import random\n",
    "#oversampling\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-GET PREPARE (data cleanup / curation, data source, privacy, preprocessing)\n",
    "-EXPLORE (plots, patterns, correlations, outliers, anomalies)\n",
    "-MODEL (train prediction, validation, tuning, scale and deployment)\n",
    "-COMMUNICATE ([....])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature engineering\n",
    "feature extraction\n",
    "feature selection : pearson correlation, wrapper method, lasso regression, explore with histogramms, scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from itertools import compress\n",
    "sfm_selector = SelectFromModel(estimator=LogisticRegression())\n",
    "sfm_selector.fit(X, y)\n",
    "list_of_selected_features = list(compress(feature_names, sfm_selector.get_support().tolist()))\n",
    "print(list_of_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "#model = DecisionTreeRegressor()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.coef_\n",
    "#importance = model.feature_importances_\n",
    "#imp_features = [f for i_f, f in enumerate(feature_names) if abs(importance[0][i_f]) < 0.05]\n",
    "importance = np.abs(importance[0])\n",
    "feature_names_s = [feature_names[f] for f in importance.argsort()]\n",
    "less_important_5 = feature_names_s[:5]\n",
    "most_important_5 = feature_names_s[:-6:-1]\n",
    "print(\"Less important features:\")\n",
    "print(less_important_5)\n",
    "print(\"Most important features:\")\n",
    "print(most_important_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot feature importance / ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIAL CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data reading\n",
    "dataset = pd.read_csv(\"final_dataset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = dataset.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA INSIGHTS - VISUALISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_input, expected_output = model_data[:, -1], model_data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test data\n",
    "x_train, x_test, y_train, y_test = None, None, None, None\n",
    "#train_test_split(model_input, expected_output, test_size=0.20, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALING - STANDARIZATION[prefer] - NORMALIZATION(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=False) # no mean?\n",
    "\n",
    "scaled_data_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "pipe = make_pipeline(scaler, scaled_data_model)\n",
    "pipe.fit(x_train, y_train)\n",
    "Pipeline(steps=[('standardscaler', scaler),\n",
    "                ('logisticregression', scaled_data_model)])\n",
    "\n",
    "pipe.score(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method selection\n",
    "\n",
    "types / number of features\n",
    "dataset size\n",
    "explainability\n",
    "class priors\n",
    "need for param tuning\n",
    "incremental training\n",
    "in/out memory training\n",
    "prediction speed\n",
    "train speed\n",
    "performance\n",
    "features indipendent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate methods (with or without scale)\n",
    "#(create func for testing training and testing speed with plots)\n",
    "models = []\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "models.append((\"LogisticRegression\",lr))\n",
    "models.append((\"KNeighbors\",KNeighborsClassifier()))\n",
    "models.append((\"DecisionTree\",DecisionTreeClassifier()))\n",
    "svm.SVC()\n",
    "models.append((\"SVC\",SVC()))\n",
    "models.append((\"Linear SVC\",LinearSVC()))\n",
    "models.append((\"SVM\",))\n",
    "models.append((\"NaiveBayes\",GaussianNB()))\n",
    "models.append((\"RandomForest\",RandomForestClassifier()))\n",
    "models.append((\"NeuralNetwork\",MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', max_iter=500)))\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name,model in models:\n",
    "    result = cross_val_score(model, x_train, y_train, cv=3, scoring = 'accuracy')\n",
    "    names.append(name)\n",
    "    results.append(result)\n",
    "    \n",
    "\n",
    "#for i in range(len(names)):\n",
    "#    print(names[i],results[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "for i in range(len(names)):\n",
    "    means.append(results[i].mean())\n",
    "plt.bar(names, means)\n",
    "plt.title(\"Accurancy comparison\")\n",
    "plt.savefig('plot2.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = \n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "space = dict()\n",
    "# space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "# space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\n",
    "# space['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "result = search.fit(x_train, y_train)\n",
    "\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "#model train\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "prediction = clf.predict(x_test)\n",
    "\n",
    "print(\"model score: \",clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', C=10, solver='newton-cg' ,max_iter=1000)\n",
    "\n",
    "#model train\n",
    "clf.fit(idf_train, output_train)\n",
    "prediction = clf.predict(idf_validation)\n",
    "\n",
    "# save the model\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "  \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#  *** INSERT TEST DATA HERE ***\n",
    "result = loaded_model.score(idf_validation, output_validation)\n",
    "print(\"loaded_model score: \",result)\n",
    "print(\"--------------------------------\")\n",
    "#  *** --------------------- ***\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"accurancy: \",metrics.accuracy_score(output_validation,prediction))\n",
    "print(\"precision: \",metrics.precision_score(output_validation,prediction,average='macro'))\n",
    "print(\"recall: \",metrics.recall_score(output_validation,prediction,average='micro'))\n",
    "print(\"f-measure: \",metrics.f1_score(output_validation,prediction,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(output_validation,prediction)\n",
    " \n",
    "plot_confusion_matrix(clf, idf_validation, output_validation)\n",
    "plt.title('confusion matrix')\n",
    "plt.savefig('plot3.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code is pulled from sklearn documentation\n",
    "\n",
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    scoring=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    scoring : str or callable, default=None\n",
    "        A str (see model evaluation documentation) or\n",
    "        a scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "\n",
    "title = \"Learning Curves for Logistic Regression (TFIDF)\"\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = LogisticRegression(penalty='l2', C=10, solver='newton-cg' ,max_iter=1000)\n",
    "plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    idf_train,\n",
    "    output_train,\n",
    "    axes=axes[:, 0],\n",
    "    ylim = (0.0,1.1),\n",
    "    cv=cv,\n",
    "    n_jobs=4,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "\n",
    "title = \"Learning Curves for Logistic Regression (BOW)\"\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = LogisticRegression(penalty='l2', C=10, solver='newton-cg' ,max_iter=1000)\n",
    "plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    bow_train,\n",
    "    output_train,\n",
    "    axes=axes[:, 1],\n",
    "    ylim = (0.0,1.1),\n",
    "    cv=cv,\n",
    "    n_jobs=4,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "\n",
    "plt.savefig('plot6.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overfitting underfitting (bias-variance) exploration with train and validaton error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********MODEL-TESTING-INSERST-CSV-TEST-FILE-HERE***********\n",
    "\n",
    "test = pd.read_csv('imdb-reviews.csv',sep = '\\t', engine = 'python')  # insert test file here\n",
    "test['review'] = test['review'].apply(remove_links)\n",
    "test['review'] = test['review'].apply(lowercase)\n",
    "test['review'] = test['review'].apply(remove_punctuation)\n",
    "test['review'] = test['review'].apply(remove_stopwords)\n",
    "test['review'] = test['review'].apply(remove_html)\n",
    "test['rating'] = test['rating'].apply(sentiment_label)\n",
    "test.drop(test[test['rating'] == None].index, inplace = True)\n",
    "\n",
    "model_data_test = test.values\n",
    "model_input_test, expected_output_test = model_data_test[:, -1], model_data_test[:, 1]\n",
    "expected_output_test = expected_output_test.astype('str')\n",
    "\n",
    "\n",
    "loaded_model = pickle.load(open('finalized_model.sav', 'rb'))\n",
    "prediction2 = loaded_model.predict(idf_test2)\n",
    "result = loaded_model.score(idf_test, expected_output_test)\n",
    "print(\"model score:\",result)\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"accurancy: \",metrics.accuracy_score(expected_output_test,prediction2))\n",
    "print(\"precision: \",metrics.precision_score(expected_output_test,prediction2,average='macro'))\n",
    "print(\"recall: \",metrics.recall_score(expected_output_test,prediction2,average='micro'))\n",
    "print(\"f-measure: \",metrics.f1_score(expected_output_test,prediction2,average='weighted'))\n",
    "\n",
    "# **************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pr curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dimentionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search space\n",
    "def objective(trial):\n",
    "    \n",
    "    params = {\n",
    "              'learning_rate': trial.suggest_categorical('learning_rate', [1e-5, 1e-4, 1e-3]),\n",
    "              'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]),\n",
    "              'n_unit': trial.suggest_categorical(\"n_unit\", [64, 128]),\n",
    "              'epochs': trial.suggest_categorical(\"epochs\", [20, 50, 80, 100]),\n",
    "              'batch_size': trial.suggest_categorical(\"batch_size\", [16, 32, 64]),\n",
    "              'loss_funct': trial.suggest_categorical(\"loss_funct\", [\"MSELoss\", \"BCELoss\"]),\n",
    "              'dropout': trial.suggest_categorical(\"dropout\", [True, False]),\n",
    "              'activationFunction': trial.suggest_categorical(\"activationFunction\", [\"LeakyReLU\", \"ReLU\"])\n",
    "             }\n",
    "    \n",
    "    model = build_model(params,trial)\n",
    "    \n",
    "    accuracy = train_and_evaluate(params,model,trial)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(params,trial):\n",
    "    \n",
    "    # number of layers in network\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "    # Define layer sizes\n",
    "    in_features = x_train.shape[1]\n",
    "    #in_features = 200\n",
    "    nodes =  params['n_unit']\n",
    "    out_features = nodes\n",
    "    D_out = 1\n",
    "    # flag to add dropout layer\n",
    "    withDropout = params['dropout']\n",
    "    activationFunction = getattr(nn, params['activationFunction'])() \n",
    "\n",
    "    for i in range(n_layers):\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(activationFunction)\n",
    "        if(withDropout):\n",
    "            layers.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "        \n",
    "        # next layer nodes ar half of previous layer\n",
    "        in_features = out_features\n",
    "        out_features = out_features // 2  \n",
    "        \n",
    "    # output layer\n",
    "    layers.append(nn.Linear(in_features,D_out))\n",
    "    layers.append(nn.Sigmoid())       \n",
    "\n",
    "    #Initialise model\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(params,trial):\n",
    "    \n",
    "    # number of layers in network\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "    # Define layer sizes\n",
    "    in_features = x_train.shape[1]\n",
    "    #in_features = 200\n",
    "    nodes =  params['n_unit']\n",
    "    out_features = nodes\n",
    "    D_out = 1\n",
    "    # flag to add dropout layer\n",
    "    withDropout = params['dropout']\n",
    "    activationFunction = getattr(nn, params['activationFunction'])() \n",
    "\n",
    "    for i in range(n_layers):\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(activationFunction)\n",
    "        if(withDropout):\n",
    "            layers.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "        \n",
    "        # next layer nodes ar half of previous layer\n",
    "        in_features = out_features\n",
    "        out_features = out_features // 2  \n",
    "        \n",
    "    # output layer\n",
    "    layers.append(nn.Linear(in_features,D_out))\n",
    "    layers.append(nn.Sigmoid())       \n",
    "\n",
    "    #Initialise model\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study.best_trial\n",
    "\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------------------\")\n",
    "print(\"Model confusion matrix: \\n\",confusion_matrix(y_validation, y_pred_list))\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Model classification report: \\n\",classification_report(y_validation, y_pred_list))\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Model accuracy: \",metrics.accuracy_score(y_validation, y_pred_list))\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "plt.plot(epoch_acc_train,'-o')\n",
    "plt.plot(epoch_acc_val,'-o')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.title('Train vs Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epoch_loss_train,'-o')\n",
    "plt.plot(epoch_loss_val,'-o')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('losses')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.title('Train vs Validation Losses')\n",
    "plt.show()\n",
    "\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(y_validation, y_probab_list)\n",
    "print(\"--------------------------------------------\")\n",
    "print('roc_auc_score for model: ', roc_auc_score(y_validation, y_probab_list))\n",
    "print(\"--------------------------------------------\")\n",
    "plt.title('Receiver Operating Characteristic - Neural Network')\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model,testloader, y_test):\n",
    "    \n",
    "    y_test = y_test.squeeze().tolist()\n",
    "    \n",
    "    y_pred_list = []\n",
    "    y_probab_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_label in testloader:\n",
    "            y_test_pred = model(X_batch)\n",
    "            y_pred_tag = torch.round(y_test_pred)\n",
    "            y_pred_list.append(y_pred_tag.numpy())\n",
    "            y_probab_list.append(y_test_pred.numpy())\n",
    "\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "    y_probab_list = [a.squeeze().tolist() for a in y_probab_list]\n",
    "    \n",
    "        \n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Model confusion matrix: \\n\",confusion_matrix(y_test, y_pred_list))\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Model classification report: \\n\",classification_report(y_test, y_pred_list))\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Model accuracy: \",metrics.accuracy_score(y_test, y_pred_list))\n",
    "    print(\"--------------------------------------------\")\n",
    "\n",
    "    \n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_probab_list)\n",
    "    print(\"--------------------------------------------\")\n",
    "    print('roc_auc_score for model: ', roc_auc_score(y_test, y_probab_list))\n",
    "    print(\"--------------------------------------------\")\n",
    "    plt.title('Receiver Operating Characteristic - Neural Network')\n",
    "    plt.plot(false_positive_rate, true_positive_rate)\n",
    "    plt.plot([0, 1], ls=\"--\")\n",
    "    plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = None\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "plt.style.use('seaborn') # use seaborn style plotting\n",
    "\n",
    "#file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_intermediate_values\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)\n",
    "plot_parallel_coordinate(study)\n",
    "optuna.visualization.plot_param_importances(study)\n",
    "optuna.visualization.plot_contour(study, params=['num_layers', 'hidden_size'])\n",
    "optuna.visualization.plot_contour(study, params=['cell_type', 'num_layers'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
