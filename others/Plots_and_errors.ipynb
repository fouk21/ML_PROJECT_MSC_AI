{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f231e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\n",
    "import shap\n",
    "X1 = test_under.drop(['CLAIMED'], axis=1)\n",
    "# X1 = X_important_train\n",
    "shap.initjs()\n",
    "# clf5.fit(x1, x1)\n",
    "\n",
    "\n",
    "\n",
    "## use the proper explainer according to the model I use\n",
    "## look link \"https://medium.com/swlh/push-the-limits-of-explainability-an-ultimate-guide-to-shap-library-a110af566a02\"\n",
    "## for better advice on which explainer to use\n",
    "\n",
    "explainer = shap.TreeExplainer(clf5, X1, feature_perturbation = \"interventional\")\n",
    "# explainer = shap.TreeExplainer(xgb, X1, feature_perturbation = \"interventional\")\n",
    "\n",
    "\n",
    "# project data point of background dataset\n",
    "shap_values = explainer.shap_values(X1, check_additivity = False)\n",
    "# obtain interaction values\n",
    "shap_interaction_values = explainer.shap_interaction_values(X1)\n",
    "# dimensions\n",
    "shap_values.shape\n",
    "shap_interaction_values.shape\n",
    "\n",
    "\n",
    "# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\n",
    "shap.force_plot(explainer.expected_value, shap_values[1,:], X1.iloc[1,:])\n",
    "\n",
    "\n",
    "\n",
    "shap.summary_plot(shap_values, X1)\n",
    "shap.summary_plot(shap_values, X1, plot_type=\"bar\")\n",
    "#shap.summary_plot(shap_interaction_values, X1, plot_type= \"compact_dot\")\n",
    "\n",
    "\n",
    "##Code taken from Dimitris, I kept this part bc it may be useful in the future \n",
    "##if we want to make a dependence plot classique\n",
    "\n",
    "# dependence_plot classique\n",
    "#shap.dependence_plot(\"Past_Requests_Disco_6M\", shap_values, features = X1,\n",
    "#display_features = X_background_display,\n",
    "# interaction_index = \"Unpaid Amount\")\n",
    "#shap.dependence_plot(\"Speed_ratio\", shap_values, features = X1,\n",
    "#display_features = X_background_display,\n",
    "# interaction_index = \"Unpaid Amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c4499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "feature_imp = pd.DataFrame(sorted(zip(clf5.feature_importances_,final_train.drop(['CLAIMED'], axis=1).columns)), \n",
    "                           columns=['Value','Feature'])\n",
    "feature_imp = feature_imp.sort_values(by=\"Value\", ascending=False)[:30]\n",
    "plt.figure(figsize=(17, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False), color = 'r')\n",
    "plt.title('LightGBM Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4c7c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335193a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8f35dab",
   "metadata": {},
   "source": [
    "### Evaluation_metrics_MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7b5ba03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.601324\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"new_uplift\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_rfr = evaluator.evaluate(rfr_preds)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "77c60b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data = 0.361591\n"
     ]
    }
   ],
   "source": [
    "evaluator_mse = RegressionEvaluator(\n",
    "    labelCol=\"new_uplift\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse_rfr = evaluator_mse.evaluate(rfr_preds)\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % mse_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d04ca961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE) on test data = 0.476589\n"
     ]
    }
   ],
   "source": [
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=\"new_uplift\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae_rfr = evaluator_mae.evaluate(rfr_preds)\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7aee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
