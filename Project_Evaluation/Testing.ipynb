{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import warnings\n",
    "\n",
    "from itables import init_notebook_mode\n",
    "init_notebook_mode(all_interactive=True)\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import auc, make_scorer, f1_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "#oversampling\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    x_train, x_test, y_train, y_test = None, None, None, None\n",
    "    model = None\n",
    "    model_path = None\n",
    "    loaded_model = None\n",
    "    classes = ['Below 10%', 'Above 10%']\n",
    "\n",
    "    def load_dataset(self):\n",
    "        self.x_train = pd.read_csv(\"x_train.csv\")\n",
    "        self.x_test = pd.read_csv(\"x_test.csv\")\n",
    "        self.y_train = pd.read_csv(\"y_train.csv\")\n",
    "        self.x_test = pd.read_csv(\"y_test.csv\")\n",
    "        return self\n",
    "\n",
    "    def train_model(self):\n",
    "        params = {}\n",
    "        model = SVC(**params)\n",
    "        model.fit(self.x_train,self.y_train)\n",
    "        self.model = model\n",
    "        print(\"Model trained successfully\")\n",
    "        return self\n",
    "\n",
    "    def load_model(self):\n",
    "        self.loaded_model = pickle.load(open(self.model_path, 'rb'))\n",
    "        print(\"model loaded successfully\")\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def model_metrics(y_true, y_pred, y_probab):\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"\\nModel Evaluation:\")\n",
    "        print(\"accurancy: \",metrics.accuracy_score(y_true,y_pred))\n",
    "        print(\"precision: \",metrics.precision_score(y_true,y_pred,average='macro'))\n",
    "        print(\"recall: \",metrics.recall_score(y_true,y_pred,average='micro'))\n",
    "        print(\"f-measure: \",metrics.f1_score(y_true,y_pred,average='weighted'))\n",
    "        print(\"--------------------------------\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_conf_matrix(classes, y_true, y_pred):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.xticks(np.arange(len(classes)) + 0.5, classes)\n",
    "        plt.yticks(np.arange(len(classes)) + 0.5, classes, rotation=0)\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_roc_curve(y_probab, y_true):\n",
    "\n",
    "        probabilities = y_probab\n",
    "\n",
    "        # Compute ROC curve and ROC AUC for each class\n",
    "        n_classes = probabilities.shape[1]\n",
    "        roc_auc = []\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true, probabilities[:, i], pos_label=i)\n",
    "            roc_auc.append(auc(fpr, tpr))\n",
    "\n",
    "        # Compute average ROC AUC score\n",
    "        average_roc_auc = sum(roc_auc) / n_classes\n",
    "\n",
    "        # Print ROC AUC score for each class\n",
    "        for i, auc_score in enumerate(roc_auc):\n",
    "            print(f\"Class {i} ROC AUC: {auc_score}\")\n",
    "\n",
    "        # Print average ROC AUC score\n",
    "        print(\"Average ROC AUC:\", average_roc_auc)\n",
    "\n",
    "        # Plot ROC curve for each class\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true, probabilities[:, i], pos_label=i)\n",
    "            plt.plot(fpr, tpr, label=f'Class {i} (ROC AUC = {roc_auc[i]:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve for Each Class')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_precission_recal_curve(self):\n",
    "        pass\n",
    "    \n",
    "    def test_model(self,load=False):\n",
    "            if load:\n",
    "                model = self.loaded_model\n",
    "            else:\n",
    "                model = self.model\n",
    "            result = cross_val_score(model, self.x_train, self.y_train, cv=6, scoring='accuracy')\n",
    "            print(\"Cross validatation Accuracy: \", np.mean(result))\n",
    "            y_pred = model.predict(self.x_test)\n",
    "            print(classification_report(self.y_test, y_pred,target_names=self.classes))\n",
    "            y_pred_probab = model.predict_proba(self.x_test)\n",
    "            self.model_metrics(self.y_test,y_pred,y_pred_probab[: , 1])\n",
    "            self.plot_conf_matrix(self.classes,self.y_test,y_pred)\n",
    "            self.plot_roc_curve(y_pred_probab,self.y_test)\n",
    "            return self\n",
    "    \n",
    "    def loaded_model_pipeline(self):\n",
    "        self.load_dataset().load_model().test_model(load=True)\n",
    "\n",
    "    def new_model_pipeline(self):\n",
    "        self.load_dataset().train_model().test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test1 = Test()\n",
    "Test2 = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test1.loaded_model_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test2.new_model_pipeline()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
